{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Be sure to restart the notebook kernel if you make changes to parseTandA\n",
    "# Re-running this cell does not re-load the module otherwise\n",
    "from helper import *\n",
    "\n",
    "# We use matplotlib for plotting. You can basically get any plot layout/style\n",
    "# etc you want with this module. I'm setting it up for basics here, meaning\n",
    "# that I want it to parse LaTeX and use the LaTeX font family for all text.\n",
    "# !! If you don't have a LaTeX distribution installed, this notebook may\n",
    "#    throw errors when it tries to create the plots. If that happens, \n",
    "#    either install a LaTeX distribution or remove/comment the \n",
    "#    matplotlib.rcParams.update(...) line.\n",
    "#    In both cases, restart the kernel of this notebook afterwards.\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "rcparams = {                      \n",
    "    \"pgf.texsystem\": \"pdflatex\",        # change this if using xetex or lautex\n",
    "    \"text.usetex\": True,                # use LaTeX to write all text\n",
    "    \"font.family\": \"lmodern\",\n",
    "    \"font.serif\": [],                   # blank entries should cause plots to inherit fonts from the document\n",
    "    \"font.sans-serif\": [],\n",
    "    \"font.monospace\": [],          \n",
    "    \"font.size\": 12,\n",
    "    \"legend.fontsize\": 12,         \n",
    "    \"xtick.labelsize\": 12,\n",
    "    \"ytick.labelsize\": 12,\n",
    "    \"pgf.preamble\": [\n",
    "        r\"\\usepackage[utf8x]{inputenc}\",    # use utf8 fonts becasue your computer can handle it :)\n",
    "        r\"\\usepackage[T1]{fontenc}\",        # plots will be generated using this preamble\n",
    "        ]\n",
    "}\n",
    "matplotlib.rcParams.update(rcparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the title dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_parse = False\n",
    "if re_parse:\n",
    "    all_titles = load_and_parse_all_titles('alltitles.txt')\n",
    "    # Save to a file, so we can load it much faster than having\n",
    "    # to re-parse the raw data.\n",
    "    np.save(\"alltitles.npy\", all_titles)\n",
    "else:\n",
    "    # Load the titles from the file.\n",
    "    # The atleast_2d is a hack for correctly loading the dictionary...\n",
    "    all_titles = np.atleast_2d(np.load(\"alltitles.npy\"))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017]\n"
     ]
    }
   ],
   "source": [
    "# Check the available years\n",
    "all_years = sorted(list(all_titles.keys()))\n",
    "print(all_years)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "titles = get_titles_for_years(all_titles, all_years)\n",
    "ngram_titles, bigrams, ngrams = get_ngram_titles(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train word2vec \n",
    "model = gensim.models.Word2Vec(ngram_titles, window=10, min_count=5, size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: \n",
      "A superconductor is similar to: \n",
      "   [*] layered_superconductor              \t (0.7740792632102966)\n",
      "   [*] superconducting                     \t (0.7646154761314392)\n",
      "   [*] superconductivity                   \t (0.7367954850196838)\n",
      "   [*] cuprate_superconductor              \t (0.7360122203826904)\n",
      "   [*] multiband_superconductor            \t (0.7169554829597473)\n",
      "   [*] unconventional_superconductor       \t (0.7168256044387817)\n",
      "   [*] cuprate                             \t (0.6782978177070618)\n",
      "   [*] superconducting_gap                 \t (0.6730670928955078)\n",
      "   [*] weyl_semimetal                      \t (0.6508858799934387)\n",
      "   [*] noncentrosymmetric_superconductor   \t (0.6402531862258911)\n",
      "Majorana is similar to: \n",
      "   [*] majorana_fermion                    \t (0.8511493802070618)\n",
      "   [*] non_abelian                         \t (0.8056158423423767)\n",
      "   [*] majorana_mode                       \t (0.7909597754478455)\n",
      "   [*] braiding                            \t (0.7504773736000061)\n",
      "   [*] majorana_edge                       \t (0.7459567189216614)\n",
      "   [*] kramer_pair                         \t (0.7449244260787964)\n",
      "   [*] floquet                             \t (0.7407909035682678)\n",
      "   [*] topologically_protected             \t (0.7405100464820862)\n",
      "   [*] andreev                             \t (0.7280244827270508)\n",
      "   [*] zero_energy                         \t (0.7266129851341248)\n",
      "Topological is similar to: \n",
      "   [*] majorana                            \t (0.7198684811592102)\n",
      "   [*] topological_insulator               \t (0.7060487270355225)\n",
      "   [*] non_abelian                         \t (0.6574746370315552)\n",
      "   [*] weyl                                \t (0.6543601155281067)\n",
      "   [*] majorana_fermion                    \t (0.6512606739997864)\n",
      "   [*] gapped                              \t (0.6338715553283691)\n",
      "   [*] topological_invariant               \t (0.6251199245452881)\n",
      "   [*] chiral                              \t (0.6240078806877136)\n",
      "   [*] floquet_topological                 \t (0.6222407221794128)\n",
      "   [*] topologically_trivial               \t (0.6166621446609497)\n",
      "\n",
      "\n",
      "Arithmetics: \n",
      "Majorana + Braiding = \n",
      "   [*] non_abelian                         \t (0.8675787448883057)\n",
      "   [*] majorana_mode                       \t (0.8489199280738831)\n",
      "particle + charge = \n",
      "   [*] electron                            \t (0.5459829568862915)\n",
      "   [*] charge_carrier                      \t (0.4906330108642578)\n",
      "electron - charge = \n",
      "   [*] unitary_transformation              \t (0.5421560406684875)\n",
      "   [*] fermion                             \t (0.5313664674758911)\n",
      "2D + electrons + magnetic field = \n",
      "   [*] landau_level                        \t (0.6418390274047852)\n",
      "   [*] rashba                              \t (0.593941867351532)\n",
      "Electron + Hole = \n",
      "   [*] carrier                             \t (0.7228261232376099)\n",
      "   [*] charge_carrier                      \t (0.6589670181274414)\n",
      "Superconductor + Topological = \n",
      "   [*] weyl_semimetal                      \t (0.7636677622795105)\n",
      "   [*] topological_insulator               \t (0.7307190895080566)\n",
      "Spin + Magnetic Field = \n",
      "   [*] antiferromagnetic                   \t (0.6644793748855591)\n",
      "   [*] magnetization                       \t (0.6556909084320068)\n"
     ]
    }
   ],
   "source": [
    "print(\"Similarity: \")\n",
    "print(\"A superconductor is similar to: \")\n",
    "for s in model.most_similar(positive=['superconductor'], topn=10):\n",
    "    print(\"   [*] {0:35} \\t ({1})\".format(s[0], s[1]))\n",
    "    \n",
    "print(\"Majorana is similar to: \")\n",
    "for s in model.most_similar(positive=['majorana'], topn=10):\n",
    "    print(\"   [*] {0:35} \\t ({1})\".format(s[0], s[1]))\n",
    "\n",
    "print(\"Topological is similar to: \")    \n",
    "for s in model.most_similar(positive=['topological'], topn=10):\n",
    "    print(\"   [*] {0:35} \\t ({1})\".format(s[0], s[1]))\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Arithmetics: \")\n",
    "print(\"Majorana + Braiding = \")\n",
    "for s in model.most_similar(positive=['majorana', 'braiding'], topn=2):\n",
    "    print(\"   [*] {0:35} \\t ({1})\".format(s[0], s[1]))\n",
    "    \n",
    "print(\"particle + charge = \")\n",
    "for s in model.most_similar(positive=['particle', 'charge'], topn=2):\n",
    "    print(\"   [*] {0:35} \\t ({1})\".format(s[0], s[1]))\n",
    "    \n",
    "print(\"electron - charge = \")\n",
    "for s in model.most_similar(positive=['electron', 'positive'], negative=['negative'], topn=2):\n",
    "    print(\"   [*] {0:35} \\t ({1})\".format(s[0], s[1]))\n",
    "    \n",
    "print(\"2D + electrons + magnetic field = \")\n",
    "for s in model.most_similar(positive=['two_dimensional', 'electron', 'magnetic_field'], topn=2):\n",
    "    print(\"   [*] {0:35} \\t ({1})\".format(s[0], s[1]))\n",
    "\n",
    "print(\"Electron + Hole = \")\n",
    "for s in model.most_similar(positive=['electron', 'hole'], topn=2):\n",
    "    print(\"   [*] {0:35} \\t ({1})\".format(s[0], s[1]))\n",
    "    \n",
    "print(\"Superconductor + Topological = \")\n",
    "for s in model.most_similar(positive=['superconductor', 'topological'], topn=2):\n",
    "    print(\"   [*] {0:35} \\t ({1})\".format(s[0], s[1]))\n",
    "    \n",
    "print(\"Spin + Magnetic Field = \")\n",
    "for s in model.most_similar(positive=['spin', 'magnetic_field'], topn=2):\n",
    "    print(\"   [*] {0:35} \\t ({1})\".format(s[0], s[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If you want to save and/or load a model:\n",
    "model.save(\"condmat-model-window-10-mincount-5-size-100\")\n",
    "#model = gensim.models.Word2Vec.load(\"condmat-model-window-10-mincount-5-size-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=500, random_state=0).fit(model.wv.syn0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sets = {}\n",
    "for l in np.unique(kmeans.labels_):\n",
    "    sets[l] = []\n",
    "for idx,l in enumerate(sorted(kmeans.labels_)):\n",
    "    sets[l].append(model.wv.index2word[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499])\n",
      "0 6974\n",
      "1 101\n",
      "2 3\n",
      "3 76\n",
      "4 173\n",
      "5 563\n",
      "6 2\n",
      "7 11\n",
      "8 20\n",
      "9 11\n",
      "10 4\n",
      "11 27\n",
      "12 31\n",
      "13 28\n",
      "14 4\n",
      "15 9\n",
      "16 30\n",
      "17 2\n",
      "18 6\n",
      "19 9\n",
      "20 4\n",
      "21 7\n",
      "22 57\n",
      "23 13\n",
      "24 1\n",
      "25 19\n",
      "26 7\n",
      "27 1\n",
      "28 22\n",
      "29 6\n",
      "30 1\n",
      "31 2\n",
      "32 31\n",
      "33 1\n",
      "34 68\n",
      "35 1\n",
      "36 565\n",
      "37 145\n",
      "38 1\n",
      "39 222\n",
      "40 9\n",
      "41 6\n",
      "42 3\n",
      "43 1\n",
      "44 1\n",
      "45 14129\n",
      "46 1\n",
      "47 10\n",
      "48 1\n",
      "49 5\n",
      "50 7\n",
      "51 30\n",
      "52 27\n",
      "53 4\n",
      "54 104\n",
      "55 2\n",
      "56 8\n",
      "57 7\n",
      "58 1\n",
      "59 1\n",
      "60 1\n",
      "61 93\n",
      "62 1\n",
      "63 6\n",
      "64 5\n",
      "65 6\n",
      "66 14\n",
      "67 3\n",
      "68 65\n",
      "69 1\n",
      "70 4\n",
      "71 1\n",
      "72 283\n",
      "73 1\n",
      "74 1\n",
      "75 4\n",
      "76 1\n",
      "77 1\n",
      "78 1\n",
      "79 1\n",
      "80 17\n",
      "81 2\n",
      "82 31\n",
      "83 2\n",
      "84 973\n",
      "85 2\n",
      "86 6\n",
      "87 4\n",
      "88 13\n",
      "89 2\n",
      "90 1\n",
      "91 24\n",
      "92 1\n",
      "93 1\n",
      "94 1\n",
      "95 1\n",
      "96 1\n",
      "97 10\n",
      "98 28\n",
      "99 1\n",
      "100 3\n",
      "101 2\n",
      "102 1\n",
      "103 1\n",
      "104 188\n",
      "105 1\n",
      "106 2\n",
      "107 2\n",
      "108 2\n",
      "109 1\n",
      "110 1\n",
      "111 14\n",
      "112 13\n",
      "113 1\n",
      "114 1\n",
      "115 1\n",
      "116 5\n",
      "117 10\n",
      "118 12\n",
      "119 1\n",
      "120 1\n",
      "121 21\n",
      "122 1\n",
      "123 1\n",
      "124 1\n",
      "125 1\n",
      "126 9\n",
      "127 1\n",
      "128 1\n",
      "129 1\n",
      "130 8\n",
      "131 1\n",
      "132 1\n",
      "133 1\n",
      "134 1\n",
      "135 1\n",
      "136 1\n",
      "137 1\n",
      "138 1\n",
      "139 1\n",
      "140 1\n",
      "141 9\n",
      "142 5\n",
      "143 159\n",
      "144 1\n",
      "145 1\n",
      "146 5\n",
      "147 52\n",
      "148 1\n",
      "149 1\n",
      "150 1\n",
      "151 1\n",
      "152 1\n",
      "153 1\n",
      "154 2\n",
      "155 1\n",
      "156 1\n",
      "157 1\n",
      "158 58\n",
      "159 1\n",
      "160 12\n",
      "161 1\n",
      "162 2\n",
      "163 3\n",
      "164 10\n",
      "165 1\n",
      "166 1\n",
      "167 1\n",
      "168 1\n",
      "169 1\n",
      "170 206\n",
      "171 1\n",
      "172 2\n",
      "173 1\n",
      "174 1\n",
      "175 5\n",
      "176 3\n",
      "177 1\n",
      "178 3\n",
      "179 7\n",
      "180 6\n",
      "181 1\n",
      "182 1\n",
      "183 497\n",
      "184 1\n",
      "185 1\n",
      "186 1\n",
      "187 1\n",
      "188 2\n",
      "189 1\n",
      "190 1\n",
      "191 2\n",
      "192 1\n",
      "193 1\n",
      "194 1\n",
      "195 1\n",
      "196 1\n",
      "197 8\n",
      "198 1\n",
      "199 1\n",
      "200 1\n",
      "201 1\n",
      "202 1\n",
      "203 2\n",
      "204 1\n",
      "205 13\n",
      "206 1\n",
      "207 1\n",
      "208 2\n",
      "209 1\n",
      "210 1\n",
      "211 1\n",
      "212 1\n",
      "213 2\n",
      "214 3105\n",
      "215 1\n",
      "216 1\n",
      "217 1\n",
      "218 1\n",
      "219 102\n",
      "220 1\n",
      "221 11\n",
      "222 4\n",
      "223 2\n",
      "224 5\n",
      "225 3\n",
      "226 53\n",
      "227 46\n",
      "228 7\n",
      "229 1\n",
      "230 1\n",
      "231 1\n",
      "232 8\n",
      "233 1\n",
      "234 1\n",
      "235 18\n",
      "236 17\n",
      "237 1\n",
      "238 1\n",
      "239 78\n",
      "240 1\n",
      "241 8\n",
      "242 1\n",
      "243 1\n",
      "244 1\n",
      "245 1\n",
      "246 1\n",
      "247 91\n",
      "248 1\n",
      "249 1\n",
      "250 1\n",
      "251 1\n",
      "252 9\n",
      "253 1\n",
      "254 27\n",
      "255 1\n",
      "256 6\n",
      "257 2\n",
      "258 1\n",
      "259 1\n",
      "260 1\n",
      "261 1\n",
      "262 1\n",
      "263 1\n",
      "264 1\n",
      "265 1\n",
      "266 1\n",
      "267 12\n",
      "268 1\n",
      "269 1\n",
      "270 1\n",
      "271 2\n",
      "272 33\n",
      "273 2\n",
      "274 12\n",
      "275 1\n",
      "276 1\n",
      "277 1\n",
      "278 1\n",
      "279 1\n",
      "280 1\n",
      "281 1\n",
      "282 1\n",
      "283 1\n",
      "284 24\n",
      "285 7\n",
      "286 1\n",
      "287 1\n",
      "288 4\n",
      "289 1\n",
      "290 8\n",
      "291 1\n",
      "292 2510\n",
      "293 175\n",
      "294 1\n",
      "295 99\n",
      "296 1\n",
      "297 1\n",
      "298 1\n",
      "299 289\n",
      "300 15\n",
      "301 1\n",
      "302 1\n",
      "303 1\n",
      "304 2\n",
      "305 1\n",
      "306 1\n",
      "307 9\n",
      "308 2\n",
      "309 1\n",
      "310 1\n",
      "311 1\n",
      "312 1\n",
      "313 1\n",
      "314 2\n",
      "315 1\n",
      "316 1\n",
      "317 1\n",
      "318 1\n",
      "319 39\n",
      "320 6\n",
      "321 1\n",
      "322 1\n",
      "323 1\n",
      "324 2\n",
      "325 34\n",
      "326 1\n",
      "327 1\n",
      "328 1\n",
      "329 1\n",
      "330 1\n",
      "331 1\n",
      "332 91\n",
      "333 2\n",
      "334 1\n",
      "335 1\n",
      "336 2\n",
      "337 1\n",
      "338 2\n",
      "339 1\n",
      "340 1\n",
      "341 1\n",
      "342 4\n",
      "343 1\n",
      "344 1\n",
      "345 1\n",
      "346 1\n",
      "347 5\n",
      "348 3\n",
      "349 88\n",
      "350 6\n",
      "351 7\n",
      "352 1\n",
      "353 217\n",
      "354 1\n",
      "355 1\n",
      "356 1\n",
      "357 1\n",
      "358 1\n",
      "359 1\n",
      "360 1315\n",
      "361 3\n",
      "362 13\n",
      "363 14\n",
      "364 1\n",
      "365 1\n",
      "366 29\n",
      "367 1\n",
      "368 1\n",
      "369 1\n",
      "370 1\n",
      "371 1\n",
      "372 3\n",
      "373 1\n",
      "374 1\n",
      "375 1\n",
      "376 1\n",
      "377 1\n",
      "378 12\n",
      "379 2\n",
      "380 1\n",
      "381 2\n",
      "382 1\n",
      "383 4\n",
      "384 1\n",
      "385 2\n",
      "386 1\n",
      "387 7\n",
      "388 1\n",
      "389 1\n",
      "390 2\n",
      "391 8\n",
      "392 1\n",
      "393 1\n",
      "394 6\n",
      "395 3\n",
      "396 9\n",
      "397 18\n",
      "398 1\n",
      "399 1\n",
      "400 771\n",
      "401 1\n",
      "402 470\n",
      "403 1\n",
      "404 1\n",
      "405 1\n",
      "406 1\n",
      "407 1\n",
      "408 1\n",
      "409 1\n",
      "410 44\n",
      "411 2\n",
      "412 3\n",
      "413 28\n",
      "414 16\n",
      "415 1\n",
      "416 1\n",
      "417 43\n",
      "418 78\n",
      "419 2\n",
      "420 4\n",
      "421 18\n",
      "422 5\n",
      "423 1\n",
      "424 2\n",
      "425 2\n",
      "426 1\n",
      "427 2\n",
      "428 1\n",
      "429 14\n",
      "430 1\n",
      "431 1\n",
      "432 1\n",
      "433 42\n",
      "434 1\n",
      "435 1\n",
      "436 1\n",
      "437 1\n",
      "438 1\n",
      "439 1\n",
      "440 1\n",
      "441 2\n",
      "442 1\n",
      "443 650\n",
      "444 1\n",
      "445 1\n",
      "446 6\n",
      "447 1\n",
      "448 1\n",
      "449 1\n",
      "450 3\n",
      "451 1\n",
      "452 1\n",
      "453 1\n",
      "454 1\n",
      "455 1\n",
      "456 1\n",
      "457 17\n",
      "458 1\n",
      "459 1\n",
      "460 1\n",
      "461 2\n",
      "462 3\n",
      "463 1\n",
      "464 1\n",
      "465 1\n",
      "466 29\n",
      "467 1\n",
      "468 1\n",
      "469 1\n",
      "470 2\n",
      "471 2\n",
      "472 1\n",
      "473 5\n",
      "474 3\n",
      "475 4\n",
      "476 1\n",
      "477 24\n",
      "478 1\n",
      "479 1\n",
      "480 10\n",
      "481 1\n",
      "482 1\n",
      "483 1\n",
      "484 1\n",
      "485 2\n",
      "486 2\n",
      "487 1\n",
      "488 1\n",
      "489 50\n",
      "490 1\n",
      "491 4\n",
      "492 12\n",
      "493 2\n",
      "494 2\n",
      "495 1\n",
      "496 5\n",
      "497 1\n",
      "498 2\n",
      "499 1\n"
     ]
    }
   ],
   "source": [
    "print(sets.keys())\n",
    "for k in sets.keys():\n",
    "    print(k, len(sets[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['freestanding_graphene', 'coexistent', 'fermi_contour', 'nonanalytic', 'lorenz', 'weak_value', 'leq_x', 'satisfiability_problem', 'are_there', 'simultaneously', 'spinel_oxide', 'oscillator_strength', 'transmon', 'microwave_photoresistance', 'valley_filter', 'nb_film', 'trial', 'screened_exchange', 'to_generate', 'minkowski', 'diffusional', 'pin', 'magnu_force', 'laser_excited', 'competing_species', 'classical_correspondence', 'paramagnon']\n"
     ]
    }
   ],
   "source": [
    "print(sets[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
